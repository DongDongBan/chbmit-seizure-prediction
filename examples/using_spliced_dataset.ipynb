{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.spliced_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "sample_rate = 256\n",
    "window_size = sample_rate * 60 # Use 60 seconds as test window_size\n",
    "batch_size = 64\n",
    "\n",
    "# Default train 70% : valid 10% : test 20%\n",
    "\n",
    "num_samples = 8000\n",
    "\n",
    "for patient in range(1, 24+1):\n",
    "    print(f\"Testing with CHB-MIT patient chb{patient:02d}\")\n",
    "    data_dir = f'<path to your chbmit dataset>/chb{patient:02d}'\n",
    "    json_path = f'<path to your chbmit segment dir>/chb{patient:02d}/segment_info.json'\n",
    "    ds_lst, _ = get_regression_datasets(data_dir, json_path, window_size, sample_rate)\n",
    "\n",
    "    spdst = SplicedDataset(ds_lst)\n",
    "    # 1. Check to see if imported SplicedDataset works well\n",
    "    n_chans = spdst[0][0].shape[0]\n",
    "    print(f\"SplicedDataset is of length={len(spdst)}, each idx yields (X=Tensor {spdst[0][0].shape}, y={type(spdst[0][1])})\")\n",
    "    \n",
    "    train_ds, valid_ds, test_ds = spdst.split_by_proportion(train_ratio=0.7, valid_ratio=0.1, test_ratio=0.2)\n",
    "    \n",
    "    # Use Callable Lambda Object to define different step strategy on different category labels.\n",
    "    step_func = lambda y: 4*window_size if y>7200 else window_size//2 # Overlap during pre/onset, non-overlap during inter\n",
    "\n",
    "    # train_iter = DataLoader(train_ds, batch_size=batch_size, num_workers=8, \n",
    "    #                         sampler=AugmentSequentialSampler(train_ds, random_offset=True, step_size=step_func)) # Expected interval 240.0s or 30.0s\n",
    "    \n",
    "    # # step_size param can be either an pure function or a callabel object which has internal state or an integer\n",
    "    # valid_iter = DataLoader(valid_ds, batch_size=batch_size, num_workers=8, \n",
    "    #                         sampler=AugmentSequentialSampler(valid_ds, random_offset=True, step_size=sample_rate//32)) # Expected interval 0.03125s\n",
    "    # test_iter  = DataLoader(test_ds , batch_size=batch_size, num_workers=8, \n",
    "    #                         sampler=AugmentSequentialSampler(test_ds, random_offset=True, step_size=sample_rate)) # Expected interval 1.0s\n",
    "\n",
    "    train_iter = SequentialGenerator(train_ds.datasets, random_offset=True)\n",
    "    valid_iter = SequentialGenerator(valid_ds.datasets, random_offset=True)\n",
    "    test_iter = SequentialGenerator(test_ds.datasets, random_offset=True)\n",
    "    \n",
    "    # 2. Check to see if DataLoader with custom sampler works well\n",
    "    x, y = train_iter.send(None); print(x.shape, y); x, y = train_iter.send(step_func(y));    print(x.shape, y);\n",
    "    x, y = valid_iter.send(None); print(x.shape, y); x, y = valid_iter.send(sample_rate//32); print(x.shape, y);\n",
    "    x, y = test_iter.send(None);  print(x.shape, y); x, y = test_iter.send(sample_rate);      print(x.shape, y);\n",
    "\n",
    "    del x, y, train_iter, valid_iter, test_iter, train_ds, valid_ds, test_ds, spdst, ds_lst # Trigger GC in advance\n",
    "\n",
    "    # 3. Check Classification.* Class as well\n",
    "    ds_lst, _ = get_classification_datasets(data_dir, json_path, window_size, sample_rate)\n",
    "    spdst = SplicedDataset(ds_lst)\n",
    "    seg_mask = 'r' * len(ds_lst) # Use all data for training: 'r' for training，'v' for valid，'t' for test，others ignore\n",
    "    train_ds, _, _ = spdst.split_by_seg(seg_mask)\n",
    "    expected_ratio_dict = { classification_label.pre: 1, \n",
    "                            classification_label.onset: 1, \n",
    "                            classification_label.inter: 2} # Expected Pre:Onset:Inter = 1:1:2\n",
    "    \n",
    "    train_iter = AugmentRandomDataLoader(ds_lst=train_ds.datasets, ratio_dict=expected_ratio_dict, batch_size=batch_size, \n",
    "                                         num_samples=num_samples, num_workers=8)\n",
    "\n",
    "    x, y = next(iter(train_iter)); print(x.shape, y)\n",
    "    y = y.tolist()\n",
    "    print(sum(k == classification_label.pre for k in y), end=' ')\n",
    "    print(sum(k == classification_label.onset for k in y), end=' ')\n",
    "    print(sum(k == classification_label.inter for k in y))\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tch21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
