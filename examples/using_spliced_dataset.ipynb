{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.spliced_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "sample_rate = 256\n",
    "window_size = sample_rate * 60 # 取一分钟为窗口长度\n",
    "batch_size = 64\n",
    "\n",
    "# Default train 70% : valid 10% : test 20%\n",
    "\n",
    "num_samples = 8000\n",
    "\n",
    "for patient in range(1, 24+1):\n",
    "    print(f\"Testing with CHB-MIT patient chb{patient:02d}\")\n",
    "    data_dir = f'<path to your chbmit dataset>/chb{patient:02d}'\n",
    "    json_path = f'<path to your chbmit segment dir>/chb{patient:02d}/segment_info.json'\n",
    "    spdst, _ = get_regression_datasets(data_dir, json_path, window_size, sample_rate)\n",
    "\n",
    "    # 1. Check to see if imported SplicedDataset works well\n",
    "    n_chans = spdst[0][0].shape[0]\n",
    "    print(f\"SplicedDataset is of length={len(spdst)}, each idx yields (X=Tensor {spdst[0][0].shape}, y={type(spdst[0][1])})\")\n",
    "    \n",
    "    train_ds, valid_ds, test_ds = spdst.split_by_proportion()\n",
    "\n",
    "    # Use Callable Lambda Object to define different step strategy on different category labels.\n",
    "    step_func = lambda y: 4*window_size if y == classification_label.inter else window_size//2\n",
    "    train_iter = DataLoader(train_ds, batch_size=batch_size, num_workers=8, \n",
    "                            sampler=AugmentSequentialSampler(train_ds, True, step_size=step_func)) # Expected interval 240.0s or 30.0s\n",
    "    valid_iter = DataLoader(valid_ds, batch_size=batch_size, num_workers=8, \n",
    "                            sampler=AugmentSequentialSampler(valid_ds, True, sample_rate//32)) # Expected interval 0.03125s\n",
    "    test_iter  = DataLoader(test_ds , batch_size=batch_size, num_workers=8, \n",
    "                            sampler=AugmentSequentialSampler(test_ds, True, sample_rate)) # Expected interval 1.0s\n",
    "    \n",
    "    # 2. Check to see if DataLoader with custom sampler works well\n",
    "    x, y = next(train_iter); print(x.shape, y)\n",
    "    x, y = next(valid_iter); print(x.shape, y)\n",
    "    x, y = next(test_iter); print(x.shape, y)\n",
    "\n",
    "    del x, y, train_iter, valid_iter, test_iter, train_ds, valid_ds, test_ds # Trigger GC in advance\n",
    "\n",
    "    # 3. Check Regression* Class as well\n",
    "    spdst, _ = get_classification_datasets(data_dir, json_path, window_size, sample_rate)\n",
    "    seg_mask = 'r' * len(spdst.datasets) # Use all data for training\n",
    "    train_ds, _, _ = spdst.split_by_seg(seg_mask)\n",
    "    weights = {classification_label.pre: 1, \n",
    "               classification_label.onset: 1, \n",
    "               classification_label.inter: 2} # Pre:Onset:Inter = 1:1:2\n",
    "    train_iter = DataLoader(train_ds, batch_size=batch_size, num_workers=8, # TODO RuntimeError: number of categories cannot exceed 2^24\n",
    "                            sampler=AugmentRandomSampler(data_source=spdst, weights=weights, num_samples=num_samples))\n",
    "    x, y = next(train_iter); print(x.shape, y)\n",
    "\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tch21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
