{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_path': '<path to your chb-mit directory> e.g ', 'clean_data_path': '<path to store clean & aligned dataset> e.g ./data_clean', 'label_output_path': '<path to store generated TOML files> e.g ./ref_labels', 'ignore_lst': []}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"config.json\", \"rt\") as f:\n",
    "    config_obj = json.load(f)\n",
    "print(config_obj)\n",
    "globals().update(config_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_id_func = lambda fn: int(fn[fn.rfind(\"_\")+1:fn.rfind(\"h\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "output_dir= clean_data_path\n",
    "directory = dataset_path\n",
    "\n",
    "with os.scandir(directory) as entries:\n",
    "    for entry in entries:\n",
    "        if entry.is_dir():\n",
    "            subdir_path = entry.path\n",
    "\n",
    "            # 在当前目录下创建同名子目录（如果不存在）\n",
    "            target_dir = os.path.join(output_dir, entry.name)\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "            with os.scandir(subdir_path) as mats:\n",
    "                data_header_lst = []\n",
    "                \n",
    "                mat_lst = []; shape_lst = []\n",
    "                for fmat in mats:\n",
    "                    if fmat.is_file and fmat.name.endswith(\".mat\"):\n",
    "                        if \"info\" in fmat.name.lower():\n",
    "                            info = scio.loadmat(fmat.path)\n",
    "                            info[\"__header__\"] = info[\"__header__\"].decode()\n",
    "                            info[\"fs\"] = info[\"fs\"].item()\n",
    "                            info[\"seizure_begin\"] = info[\"seizure_begin\"].squeeze().tolist()\n",
    "                            info[\"seizure_end\"] = info[\"seizure_end\"].squeeze().tolist()\n",
    "                            info_path = os.path.join(target_dir, fmat.name[:-3]+\"json\")\n",
    "                        else: \n",
    "                            mat_lst.append(fmat)\n",
    "                            temp = scio.loadmat(fmat.path) # 第一遍读取：这里原理上只用读取文件头就可以，但是python生态没有提供方便读取老版本MAT文件头的API\n",
    "                            shape_lst.append(temp[\"EEG\"].shape)\n",
    "\n",
    "                mat_lst.sort(key=lambda e: get_id_func(e.name))\n",
    "                sum_shape = (shape_lst[0][0], sum(s[1] for s in shape_lst))\n",
    "                result = np.memmap(os.path.join(target_dir, f'{entry.name}_eeg.npy'),  dtype='float32', mode='w+', shape=sum_shape)\n",
    "                cur_idx = 0\n",
    "                for fmat in mat_lst:\n",
    "                        print(f\"Reading {fmat.path}\")\n",
    "                        data = scio.loadmat(fmat.path) # 第二遍读取：用于实际数据处理\n",
    "                        result[:, cur_idx:cur_idx+data[\"EEG\"].shape[1]] = data[\"EEG\"][:]\n",
    "                        result.flush(); cur_idx += data[\"EEG\"].shape[1]\n",
    "                        data[\"__header__\"] = data[\"__header__\"].decode()\n",
    "                        data[\"__shape__\"] = data[\"EEG\"].shape\n",
    "                        del data[\"EEG\"]\n",
    "                        data_header_lst.append((fmat.name, data))\n",
    "                        \n",
    "                # data_header_lst.sort(key=lambda t: get_id_func(t[0]))\n",
    "                with open(os.path.join(target_dir, entry.name+\"_data.json\"), \"wt\") as f:\n",
    "                    json.dump(data_header_lst, f, indent=4)\n",
    "                \n",
    "                info[\"file\"] = f'{entry.name}_eeg.npy' # result.filename; \n",
    "                info[\"shape\"] = sum_shape\n",
    "                with open(info_path, \"wt\") as f:\n",
    "                    json.dump(info, f, indent=4)\n",
    "            # print(f\"Created directory: {target_dir}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[en_US] TODO\n",
    "\n",
    "[zh_CN]（可选步骤）测试上面生成的清洗数据集的正确性 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID07: 2 days, 20:57:06, 75 channels, 4 onset(s)\n",
      "ID03: 6 days, 14:23:01, 64 channels, 4 onset(s)\n",
      "ID01: 12 days, 5:21:33, 88 channels, 2 onset(s)\n",
      "ID04: 1 day, 16:41:37, 32 channels, 14 onset(s)\n",
      "ID14: 6 days, 17:23:32, 24 channels, 60 onset(s)\n",
      "ID18: 8 days, 13:04:22, 42 channels, 5 onset(s)\n",
      "ID09: 1 day, 16:52:35, 48 channels, 27 onset(s)\n",
      "ID06: 6 days, 1:59:12, 32 channels, 8 onset(s)\n",
      "ID08: 5 days, 23:47:14, 61 channels, 70 onset(s)\n",
      "ID15: 8 days, 3:55:00, 98 channels, 2 onset(s)\n",
      "ID16: 7 days, 9:03:34, 34 channels, 5 onset(s)\n",
      "ID10: 1 day, 18:23:24, 32 channels, 17 onset(s)\n",
      "ID12: 7 days, 23:25:00, 56 channels, 9 onset(s)\n",
      "ID11: 8 days, 20:13:02, 32 channels, 2 onset(s)\n",
      "ID17: 5 days, 9:35:57, 60 channels, 2 onset(s)\n",
      "ID02: 9 days, 19:13:05, 66 channels, 2 onset(s)\n",
      "ID13: 4 days, 7:59:26, 64 channels, 7 onset(s)\n",
      "ID05: 4 days, 13:24:22, 128 channels, 4 onset(s)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import json\n",
    "output_dir=clean_data_path\n",
    "\n",
    "meta_info = {}\n",
    "with os.scandir(output_dir) as pats:\n",
    "    for pat in pats:\n",
    "        if pat.is_dir():\n",
    "            meta_info[pat.name] = pat_meta = {}\n",
    "            with os.scandir(pat.path) as jsons:\n",
    "                # assert len(jsons) == 2\n",
    "                sum_td = 0\n",
    "                for fjson in jsons:\n",
    "                    if fjson.name == (pat.name+\"_eeg.dat\"):\n",
    "                        lf = np.memmap(fjson.path, dtype=np.float32, mode=\"r\")\n",
    "                        continue\n",
    "\n",
    "                    with open(fjson.path, \"rt\") as f:\n",
    "                        obj = json.load(f)\n",
    "                        if fjson.name == (pat.name+\"_info.json\"):\n",
    "                            pat_meta[\"fs\"] = obj[\"fs\"]\n",
    "                            assert len(obj[\"seizure_begin\"]) == len(obj[\"seizure_end\"])\n",
    "                            assert all([obj[\"seizure_begin\"][k] > obj[\"seizure_end\"][k-1] for k in range(1, len(obj[\"seizure_begin\"]))])\n",
    "                            assert all(map(lambda t: t[0] < t[1], zip(obj[\"seizure_begin\"], obj[\"seizure_end\"])))\n",
    "                            pat_meta[\"beg\"] = obj[\"seizure_begin\"]; pat_meta[\"end\"] = obj[\"seizure_end\"]\n",
    "                            pat_meta[\"shape\"] = obj[\"shape\"]\n",
    "                        elif fjson.name == (pat.name+\"_data.json\"): \n",
    "                            assert len(obj) == get_id_func(obj[-1][0])\n",
    "                            common_shape = obj[0][1][\"__shape__\"]\n",
    "                            for idx, header in zip(range(1, len(obj)+1), obj):\n",
    "                                assert get_id_func(header[0]) == idx\n",
    "                                if idx != len(obj):\n",
    "                                    assert header[1][\"__shape__\"] == common_shape\n",
    "                                else:\n",
    "                                    assert header[1][\"__shape__\"][0] == common_shape[0]\n",
    "                                sum_td += header[1][\"__shape__\"][1]\n",
    "                            first_mat_obj = scio.loadmat(os.path.join(directory, pat.name, obj[0][0])) \n",
    "                            last_mat_obj = scio.loadmat(os.path.join(directory, pat.name, obj[-1][0]))\n",
    "                            pat_meta[\"nch\"] = common_shape[0]\n",
    "                        else:\n",
    "                            raise AssertionError(f\"Unknown Json File {fjson.path}\")\n",
    "                pat_meta[\"len\"] = sum_td / pat_meta[\"fs\"]\n",
    "                assert pat_meta[\"beg\"][0] >= 0 and pat_meta[\"end\"][-1] <= pat_meta[\"len\"]\n",
    "                assert lf.size == pat_meta['nch'] * sum_td\n",
    "\n",
    "                lf = lf.reshape(pat_meta[\"shape\"])\n",
    "                assert np.all(np.equal(lf[:, :first_mat_obj[\"EEG\"].shape[1]], first_mat_obj[\"EEG\"]))\n",
    "                assert np.all(np.equal(lf[:, -(last_mat_obj[\"EEG\"].shape[1]):], last_mat_obj[\"EEG\"]))\n",
    "                del lf, first_mat_obj, last_mat_obj\n",
    "\n",
    "                print(f\"{pat.name}: {datetime.timedelta(seconds=pat_meta['len'])}, {pat_meta['nch']} channels, {len(pat_meta['beg'])} onset(s)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
